---
title: "p8105_hw3_mjn2142"
author: "Matthew Neky"
date: "10/18/2021"
output: github_document
---

### Setup Code

```{r setup, message = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```

The instacart dataset is quite large, with 15 columns and 1384617 rows. The table is structured with each row corresponding to a single item ordered from instacart and the associated information for that single item. Some key variables are "order_id," "user_id," "product_name," "aisle_id," department_id," "aisle," and "department." For example, the first item in the table is given the number 1, order_id is 1, user_id is 112108, product_name is Bulgarian Yogurt, aisle_id is 120, department_id is 16, aisle is yogurt, and department is dairy eggs. As made clear by this example, important variables take both numeric and alphabetic forms. Certain columns will also be made up of distinct data while others will have a few highly repeated categories. 

```{r}
aisle_df = instacart %>% 
  group_by(aisle) %>% 
  summarize(aisle_obs = n()) %>% 
  arrange(desc(aisle_obs))
```

There are 134 aisles with "fresh vegetables," "fresh fruits," and "packaged vegetables fruits" as the three most popular in terms of items ordered.

```{r}
aisle_plot = aisle_df %>% 
  filter(
    aisle_obs > 10000
  ) %>% 
  ggplot(aes(x = aisle, y = aisle_obs)) +
  geom_point()
```

Most of the data points are below 40,000 in this plot, with only 5 aisles that have more than 40,000 items ordered

```{r}
popular_item_df = 
  instacart %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")
  ) %>% 
  group_by(aisle) %>% 
  count(product_name, name = "product_count") %>% 
  mutate(
    items_rank = min_rank(desc(product_count))
  ) %>% 
  filter(items_rank <= 3) %>% 
  select(-items_rank)

  knitr::kable(popular_item_df, format = "html")
```

The most popular item in the baking ingredients aisle is Light Brown Sugar (499 orders), the most popular in dog food care is "Snack Sticks Chicken & Rice Recipe Dog Treats" (30 orders), and the most poular in packaged vegetables fruits is "Organic Baby Spinach" (9784 orders).

```{r, message = FALSE}
apple_icecream_df = instacart %>% 
  mutate(
    order_dow =
      recode(
      order_dow,
      "0" = "Sunday",
      "1" = "Monday",
      "2" = "Tuesday",
      "3" = "Wednesday",
      "4" = "Thursday",
      "5" = "Friday",
      "6" = "Saturday"
    )
  ) %>% 
  group_by(product_name, order_dow) %>% 
  filter(
    product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")
    ) %>% 
  summarize(mean_hour_day = mean(order_hour_of_day)) %>%
  mutate(
    day_of_week = ordered(order_dow, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  ) %>% 
  arrange(day_of_week) %>% 
  select(-order_dow) %>% 
  pivot_wider(
    names_from = day_of_week,
    values_from = mean_hour_day
  )

knitr::kable(apple_icecream_df, format = "html")
```

For coffee ice cream, the mean hour of ordering is later from Monday through Thursday than Friday through Sunday. Pink Lady Apples don't appear to present any trend for mean times at which they are ordered.

### Problem 2

```{r}
data("brfss_smart2010")

brfss_df = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(
    topic == "Overall Health",
    response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")
    ) %>% 
  mutate(
    response =
      recode(
        response,
        "Poor" = "1",
        "Fair" = "2",
        "Good" = "3",
        "Very good" = "4",
        "Excellent" = "5"
        )
  ) %>% 
      arrange(response) %>%  
  mutate(
        response = 
        recode(
        response,
        "1" = "Poor",
        "2" = "Fair",
        "3" = "Good",
        "4" = "Very good",
        "5" = "Excellent"
    )
  )
```

```{r}
two_thousand_two_df = brfss_df %>% 
  filter(year == "2002") %>% 
  group_by(locationabbr) %>% 
  summarize(location_obs = n()) %>% 
  filter(location_obs >= 7)

two_thousand_ten_df = brfss_df %>% 
  filter(year == "2010") %>% 
  group_by(locationabbr) %>% 
  summarize(location_obs = n()) %>% 
  filter(location_obs >= 7)
```

In 2002 ony 36 states had 7 or more locations observed. In 2010, that number went up to 45 states. Some states saw increases in the number of locations observed, but some also saw decreases, so it would be inappropriate to say based on this table alone there was a trend in terms of increases or decreases in numbers of locations observed per state.

```{r, warning = FALSE, message = FALSE}
excellent_df = brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(year, locationabbr) %>% 
  summarize(data_value_mean = mean(data_value)) %>% 
  ggplot(aes(x = year, y = data_value_mean, group = locationabbr, color = locationabbr)) +
  geom_point() +
  geom_line(alpha = 0.5) +
  theme(legend.position = "right")

ggsave("excellent_plot.png")
knitr::include_graphics("excellent_plot.png")
```

This spaghetti plot shows that values will both increase as well as decrease across the years for a single state.

```{r, message = FALSE}
ny_df = brfss_df %>% 
  filter(
    locationabbr == "NY",
    year == "2006" | year == "2010"
  ) %>% 
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))
    ) %>% 
  ggplot(aes(x = response, y = data_value, color = year)) +
  geom_point(alpha = 0.5) +
  facet_grid(.~year)

ggsave("ny_plot.png")
knitr::include_graphics("ny_plot.png")
```

The two years show similar trends, with "Good" and "Very good" being the most popular responses, "Excellent" being in the middle, and "Fair" and "Poor" being the least popular, in terms of the data_value metric.

### Problem 3

Tidying accelerometer data

```{r}
accel_df = read_csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    week_classification = ifelse(day == "Saturday" | day == "Sunday", "Weekend", "Weekday")
  ) %>% 
  relocate(week, day_id, day, week_classification)
```

Create aggregate variable

```{r}
aggregate_accel_df = accel_df %>% 
  mutate(
    aggregate_activity = select(., activity_1:activity_1440) %>% rowSums(na.rm = TRUE)
  ) %>% 
  relocate(week, day_id, day, week_classification, aggregate_activity)
```

ADD TREND COMMENTS.

Accelerometer plot

```{r, message = FALSE}
accel_plot = accel_df %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    values_to = "accel_value"
    ) %>% 
  ggplot(aes(x = minute_of_day, y = accel_value, color = day)) +
  geom_point(alpha = 0.5) +
  geom_line()

ggsave("accel_plot.png")
knitr::include_graphics("accel_plot.png")
```


